---
title: "Random Forest"
author: "Jill Deines"
date: "April 4, 2017"
output: 
  html_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=FALSE, cache.path='cache/1.18_rrbRandFor/',
               fig.path='figure/1.18_rrbRandFor_test4/')
```

Goal: Run a variable importance test on a random forest of my training points

Secondary goal unlikely to be obtained: use more functions below, less copy/pasting. Oh well, we can't all be beautiful.

update 4/4/2017: This version uses the annual-based classifier only to slim down this mess.

Update: 2/26/2017 - Classify the MidRep validation points with R's random forest tree and compare with GEE's classification. Use GEE accuracy for 'test3_randomForets_500' for this, prior to any cleaning

Update 3/7/2017 - apply variable importance reduction to optimize classification on midrep validatin points

Update 3/21/2017 - run the R random forest on the test dataset points

randomForest package implementation:
 reference implementation based on CART trees
(Breiman, 2001; Liaw and Wiener, 2008)
â€“ for variables of different types: biased in favor of
continuous variables and variables with many categories
(Strobl, Boulesteix, Zeileis, and Hothorn, 2007)


**R Packages Needed**

```{r packages, message=FALSE, echo=TRUE}
library(randomForest)
library(varSelRF)
library(caret)
library(knitr) # for 'kable' table display
source('functions/prettyConfusion.R')
source('functions/testToConfusion.R')
source('functions/testToConfusion_GEE.R')
```

## Load Data
Load training point data used in GEE classification. These were processed in 1.16_trainingPoints_RRB.Rmd, which involved combining 2010 and 2012 points and removing extraneous columns, so that only the variables and a masterType and masterNum columns remained. I wrote out a csv version of the final kml to be used here.

```{r loadData}
# load kml of combined 2010/2012 training points
trainingFolder <- 'C:/Users/deinesji/Dropbox/1PhdJill/hpa/irrigation/data/GIS/training'
trainingName <- '2010_2012_COMBINED_training_12class_v2.csv'

points <- read.csv(paste0(trainingFolder, '/', trainingName))

# re-class the master types into 3 categories: irrigated, dryland crop, noncrop
typeConverter <- data.frame(masterType = unique(points$masterType),
                            classes = c('irrigated','dryland','irrigated','dryland',
                                        'irrigated','dryland','irrigated','dryland',
                                        'irrigated','dryland','noncrop','noncrop'))
points2  <- merge(points, typeConverter)

# remove extra columns
points3 <- points2[ , -which(names(points2) %in% c('masterType','masterNum'))]

# make a dataset without dryland soy
pointsNoSoy <- points2[points2$masterType != 'soy_dryland',]
pointsNoSoy2 <- pointsNoSoy[ , -which(names(pointsNoSoy) %in%
                                        c('masterType','masterNum'))]

# make an annual variable dataset without dryland soy
columnsWanted <- c('EVI_max_14','EVI_range_14','GI_max_14','GI_range_14',
                   'NDVI_max_14','NDVI_range_14','NDWI_max_14','NDWI_range_14',
                   'aridity','b1','b1_1','greenArid','ndwi_gi','pdsi_ann',
                   'pdsi_grow','pr_ann','pr_early','pr_grow','pr_paw','slope_mean',
                   'classes')
annualNoSoy <- pointsNoSoy2[, columnsWanted]
```

## Random Forest

### Annual random forest
Apply a random forest that mimics my GEE classification (500 trees, no dryland soy class)

This dataset doesn't include dryland soy, nor any monthly variables (extracted above in the load data chunk)

```{r rf500_annual, fig.height=6}
# run a random forest on all variables
set.seed(415)
fit500.ann <- randomForest(x = annualNoSoy[,1:20], # all variable columns 
                    y = as.factor(annualNoSoy$classes),
                    ntree = 500,
                    importance = TRUE)

# look at variable importance
varImpPlot(fit500.ann)
VI_F.ann <- importance(fit500.ann)[,'MeanDecreaseGini']
par(mar=c(7,4,4,2))
barplot(VI_F.ann/sum(VI_F.ann), las=3)

```


### Play with variable reduction
without dryland soy. Here are the "most important" variables.

```{r varReduction_noDsoy, cache=TRUE}
# annual
atest <- varSelRF(xdata = annualNoSoy[,1:20], whole.range=F,
                    Class = as.factor(annualNoSoy$classes),
                    ntree = 500)
atest
```


## Test Datasets: Compare accuracy: GEE & R 
Checked after settling on the random forest, 500 tree, annual variables (20 variables) method, run without soy training points. This gives accuracy from the R classification; accuracies will also be carried out on the GEE outputs (including 1x majority filter and interannual cleaning - see 5.055_Confusion Tables_Final_test4_randFor.Rmd).

### R-based Annual classifier
on the full test point datasets for 2002 and 2015

```{r testPoints_annual3_all}
# load points and variable data
vpointFolder <- 'C:/Users/deinesji/Google Drive/GEE_validation/validationSets'
p2015 <- read.csv(paste0(vpointFolder, 
                         '/2015_data_randAll_1250_jmd_annual.csv'))
p2002 <- read.csv(paste0(vpointFolder,
                         '/2002_data_NE_randAll_1050_jmd_annual.csv'))

# 2015, certainty level 1 & 2, 3 classes, annual
c2015_all_3_ann <- testToConfusion(dataset = p2015, numClasses = 3, 
                                    certainty = 2, classifier = fit500.ann)
kable(c2015_all_3_ann, digits=1)


# 2002, certainty level 1 & 2, 3 classes, annual
c2002_all_3_ann <- testToConfusion(dataset = p2002, numClasses = 3, 
                                    certainty = 2, classifier = fit500.ann)
kable(c2002_all_3_ann, digits=1)
```

And binary confusion tables 

```{r testPoints_annual2_all}
# 2015, certainty level 1 & 2, 2 classes, annual
c2015_all_2_ann <- testToConfusion(dataset = p2015, numClasses = 2, 
                                    certainty = 2, classifier = fit500.ann)
kable(c2015_all_2_ann, digits=1)


# 2002, certainty level 1 & 2, 2 classes, annual
c2002_all_2_ann <- testToConfusion(dataset = p2002, numClasses = 2, 
                                    certainty = 2, classifier = fit500.ann)
kable(c2002_all_2_ann, digits=1)
```

### GEE classifier
Just do binary

```{r compare, cache=TRUE}
# load GEE accuracy tables locations
accuDir <- 'C:/Users/deinesji/Google Drive/GEE_tableExports/Accuracy_PointsforConfusionTables_test4_randFor'
g2002 <- '2002_RRB_NE_annual_confusionData_RRB_test4_randFor_uncleaned_500tree_allClasses_annual.csv'
g2015 <- '2015_RRB_annual_confusionData_RRB_test4_randFor_uncleaned_500tree_allClasses_annual.csv'

# get confusion tables
g2002.con <-  testToConfusionGee(datadir = accuDir, dataname = g2002,
                                 bandName = 'classification', certainty = 2)
kable(g2002.con, digits=1)

# get confusion tables
g2015.con <-  testToConfusionGee(datadir = accuDir, dataname = g2015,
                                 bandName = 'classification', certainty = 2)
kable(g2015.con, digits=1)
```

it looks like 2 different for 2002: 99.8% accurate (1 - 2/2013)

## Compare: validation datasets (middle republican)

### Make predictions in R
This uses the R random forest classifier created above to predict the classes of the MidRep validation points

```{r predictMidRep}
# load points and variable data
vpointFolder <- 'C:/Users/deinesji/Google Drive/GEE_validation/validationSets'
p2007 <- read.csv(paste0(vpointFolder,'/2007_test3_variables_midRep_all.csv'))
p2010 <- read.csv(paste0(vpointFolder,'/2010_test3_variables_midRep_all.csv'))

# par down dfs
#unique(p2007$certainty)
p2007 <- p2007[ , -which(names(p2007) %in% c('.geo'))]
p2010 <- p2010[ , -which(names(p2010) %in% c('.geo'))]


# run predictions/classification without dryland soy in classifier
p2007$randFor <- predict(fit500.ann, newdata = p2007, type='response')
p2010$randFor <- predict(fit500.ann, newdata = p2010, type='response')

# 2007 R no soy
r2007_3_ns <- makeConfusion(p2007$randFor, p2007$class)
kable(r2007_3_ns, digits=1)

# 2010 R no soy
r2010_3_ns <- makeConfusion(p2010$randFor, p2010$class)
kable(r2010_3_ns, digits=1)
```

### assess validation points (middle republican)
Use the accuracy assessment exports from GEE validation to compare results between GEE and Rrrr. These currently use the uncleaned classifications from GEE, since I can't replicate that in R.

```{r compareMidrep, cache=TRUE}
# load GEE accuracy tables
accuDir <- 'C:/Users/deinesji/Google Drive/GEE_tableExports/Accuracy_PointsforConfusionTables_test4_randFor'
g2007 <- read.csv(paste0(accuDir,
        '/2007_MidRep_confusionData_RRB_test4_randFor_uncleaned_500tree_allClasses_annual.csv'))
g2010 <- read.csv(paste0(accuDir,
        '/2010_MidRep_confusionData_RRB_test4_randFor_uncleaned_500tree_allClasses_annual.csv'))

g2007 <- g2007[ , -which(names(g2007) %in% c('.geo'))]
g2010 <- g2010[ , -which(names(g2010) %in% c('.geo'))]

# extract important columns from r dataset
rColNamesWanted <- c('system.index','class','classNum','randFor')
p2007.less <- p2007[,rColNamesWanted]
p2010.less <- p2010[,rColNamesWanted]

# combine
p2007.less$system.index <- as.character(p2007.less$system.index)
p2010.less$system.index <- as.character(p2010.less$system.index)

g2007$system.index <- as.character(g2007$system.index)
g2010$system.index <- as.character(g2010$system.index)

geeR.2007 <- merge(g2007, p2007.less, by = 'system.index',all=F)
geeR.2010 <- merge(g2010, p2010.less, by = 'system.index',all=F)

# make sure the system index is an ok column to merge on
sum(!(geeR.2007$classNum.x == geeR.2007$classNum.y))

# convert GEE's classifications from numbers to words
key <- data.frame(classification = c(0,1,2),
                  gee_rf = c('dryland','irrigated','noncrop'))

geeR.2007 <- merge(geeR.2007, key)
geeR.2010 <- merge(geeR.2010, key)

# quick check for matches between gee and R
sum(!(geeR.2007$gee_rf == geeR.2007$randFor))
sum(!(geeR.2010$gee_rf == geeR.2010$randFor))

# give percentages
(nrow(geeR.2007) - sum(!(geeR.2007$gee_rf == geeR.2007$randFor))) / nrow(geeR.2007)
(nrow(geeR.2010) - sum(!(geeR.2010$gee_rf == geeR.2010$randFor))) / nrow(geeR.2010)
```

Break it into a confusion table to better compare accuracies

```{r confusion3, cache=TRUE}
# 2007 GEE: make and display confusion table
gee2007_3 <- makeConfusion(geeR.2007$gee_rf, geeR.2007$class.x)
kable(gee2007_3, digits=1)

# 2007 R: make and display confusion table
r2007_3 <- makeConfusion(geeR.2007$randFor, geeR.2007$class.x)
kable(r2007_3, digits=1)

# 2010 GEE
gee2010_3 <- makeConfusion(geeR.2010$gee_rf, geeR.2010$class.x)
kable(gee2010_3, digits=1)

# 2010 R: make and display confusion table
r2010_3 <- makeConfusion(geeR.2010$randFor, geeR.2010$class.x)
kable(r2010_3, digits=1)
```


#### Two Classes (binary classification)
Not awful, but check to see if most of the confusion is between noncrop/dryland

```{r compare2class, cache=TRUE}
# make keys to convert classifications from 2 classes to 1
key2class.1 = data.frame(gee_rf = c('dryland','irrigated','noncrop'),
                         gee_rf2 = c('notirrigated','irrigated','notirrigated'))
key2class.2 = data.frame(randFor = c('dryland','irrigated','noncrop'),
                         randFor2 = c('notirrigated','irrigated','notirrigated'))
key2class.3 = data.frame(class.x = c('dryland','irrigated','noncrop'),
                         class2 = c('notirrigated','irrigated','notirrigated'))

geeR.2007 <- merge(geeR.2007, key2class.1)
geeR.2007 <- merge(geeR.2007, key2class.2)
geeR.2007 <- merge(geeR.2007, key2class.3)

geeR.2010 <- merge(geeR.2010, key2class.1)
geeR.2010 <- merge(geeR.2010, key2class.2)
geeR.2010 <- merge(geeR.2010, key2class.3)

# quick check for matches between gee and R
sum(!(geeR.2007$gee_rf2 == geeR.2007$randFor2))
sum(!(geeR.2010$gee_rf2 == geeR.2010$randFor2))

# give percentages
(nrow(geeR.2007) - sum(!(geeR.2007$gee_rf2 == geeR.2007$randFor2))) / nrow(geeR.2007)
(nrow(geeR.2010) - sum(!(geeR.2010$gee_rf2 == geeR.2010$randFor2))) / nrow(geeR.2010)
```

That does improve the "consistency" between programs. Or likely, random seeds.

Check to see which is more accurate via confusion table

```{r confusion2, cache=TRUE}
# 2007 GEE: make and display confusion table
gee2007_2 <- makeConfusion2(geeR.2007$gee_rf2, geeR.2007$class2)
kable(gee2007_2, digits=1)

# 2007 R: make and display confusion table
r2007_2 <- makeConfusion2(geeR.2007$randFor2, geeR.2007$class2)
kable(r2007_2, digits=1)

# 2010 GEE
gee2010_2 <- makeConfusion2(geeR.2010$gee_rf2, geeR.2010$class2)
kable(gee2010_2, digits=1)

# 2010 R: make and display confusion table
r2010_2 <- makeConfusion2(geeR.2010$randFor2, geeR.2010$class2)
kable(r2010_2, digits=1)
```

